# Отчет по проекту: Виртуальный Ассистент для Квалификации Лидов

## Введение

В рамках данного проекта была выполнена разработка бота, который квалифицирует входящих лидов, предоставляет первичную консультацию, собирает контактные данные и передает их менеджеру. Проект состоит из двух основных частей: реализация логики бота и создание веб-интерфейса для взаимодействия с ботом.

## Техническая Реализация

### 1. Логика Виртуального Ассистента

Для реализации логики бота был создан Python файл `llm.py`, который использует библиотеку `LangChain` и модель `DeepInfra`.

#### Основные компоненты:

* **Шаблон Промпта:**

```python
  template_prompt = """
  "Ты – виртуальный ассистент по имени Гоша. Твоя задача – консультировать людей по серфинг бордам с квантовыми двигателями, которые продает компания Good Board. Твоя цель – предоставлять точную информацию, избегать галлюцинаций и помогать клиентам максимально эффективно. 

  Твоя инструкция:
  1. Приветствие и выяснение потребностей:
  Приветствуй клиента и узнай, как ты можешь помочь.
  Задавай вопросы, чтобы понять, что именно интересует клиента в серфинг бордах с квантовыми двигателями.
  2. Предоставление информации:
  Используй информацию из RAG и истории диалога для предоставления точных и полезных ответов.
  Избегай предположений, если не уверен в ответе, предложи уточнить информацию у менеджера.
  3. Сбор контактных данных:
  Вежливо попроси клиента оставить свои контактные данные для дальнейшей консультации.
  Собери имя, email и номер телефона клиента.
  4. Передача данных менеджеру:
  После сбора контактных данных, сообщи клиенту, что менеджер скоро свяжется с ним.
  Передай собранные данные менеджеру для дальнейшей обработки.

  Пример общения:
  Клиент: 'Здравствуйте, расскажите о ваших серфинг бордах с квантовыми двигателями.'
  Ответ: 'Здравствуйте! Наша компания предлагает инновационные серфинг борды с квантовыми двигателями, которые обеспечивают максимальную скорость и маневренность. Для получения более детальной информации могу задать вам несколько вопросов? [вставить релевантную информацию из RAG].'
  Клиент: 'Конечно, задавайте.'
  Ответ: 'Отлично! Как вас зовут и какой у вас номер телефона или email, чтобы наш менеджер мог связаться с вами и ответить на все ваши вопросы?'

  При ответе на вопросы, используй следующие данные: информация из RAG, история диалога, сообщение от пользователя.

  {human_input}

  Ответь на сообщение пользователя.
  Ответ:
  """
```

* **Класс `GSpaceBot_3`:**
  Этот класс реализует основные функции бота:
  * Инициализация модели LLM.
  * Создание эмбеддингов с использованием `SentenceTransformer`.
  * Инициализация базы данных Chroma DB для хранения и поиска релевантной информации.
  * Метод `retrieve_relevant_documents` для поиска релевантных документов.
  * Метод `get_answer` для формирования ответа на основе пользовательского ввода, информации из базы данных и истории диалога.

```python
  class GSpaceBot_3():
      def __init__(self):
          llm = DeepInfra(model_id="meta-llama/Meta-Llama-3-70B-Instruct")
          llm.model_kwargs = {
              "temperature": 0.4,
              "repetition_penalty": 1,
              "max_new_tokens": 250,
              "top_p": 0.9,
              "top_k": 0
          }

          self.model = SentenceTransformer('all-MiniLM-L6-v2')
          client = chromadb.PersistentClient(path="./chromadb")
          self.collection = client.get_collection("gboard_v7")

          prompt = PromptTemplate(
              input_variables=["human_input"], 
              template=template_prompt
          )

          self.llm_chain = LLMChain(
              llm=llm,
              prompt=prompt,
              verbose=True,
              memory=ConversationBufferWindowMemory(k=10),
          )

      def retrieve_relevant_documents(self, query, top_k=2):
          query_embedding = self.model.encode([query])[0]
          results = self.collection.query(query_embedding.tolist(), n_results=top_k)
          retrieved_texts = [result['text'] for result in results['metadatas'][0]]
          return "\n".join(retrieved_texts)

      def get_answer(self, user_input):
          retrieved_info = self.retrieve_relevant_documents(user_input)
          history = self.llm_chain.memory.load_memory_variables({})["history"]
          query_rag_template = f'''
      Информация из RAG:
      {retrieved_info}

      История диалога:
      {history}

      Сообщение от пользователя:
      {user_input}

          '''
          response = self.llm_chain.predict(human_input=query_rag_template)
        
          self.llm_chain.memory.chat_memory.messages[-2].content = user_input
        
          def format_answer(answer):
              answer = answer.replace('\n', '')
              answer = answer.replace('"', '')
              answer = answer.replace("'", '')
              answer = answer.replace('#', '')
              answer = answer.replace('`', '')

              answer = answer.strip()
              return answer
        
          f_answer = format_answer(response)
          return f_answer

  if __name__ == "__main__":
      pass 
```

### 2. Веб-Сервер и Интерфейс

Для взаимодействия с ботом через веб-интерфейс был создан Flask сервер. Сервер принимает POST-запросы на эндпоинт `/api`, обрабатывает их с использованием созданного бота и возвращает ответы.

Файл `app.py`:

```python
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from flask_cors import cross_origin
from llm import GSpaceBot_3

app = Flask(__name__)
cors = CORS(app, resources={r"/process": {"origins": "*"}})
app.config['CORS_HEADERS'] = 'Content-Type'

start = False
model = None

@app.route('/api', methods=['POST'])
@cross_origin(origin='*',headers=['Content-Type','Authorization'])
def predict():
    global start, model
    data = request.get_json()
    chat = data['messages']
    print(chat)
  

    if not start:
        start = True
        model = GSpaceBot_3()      
        response = model.get_answer(chat)
        return jsonify({'response': response})
    response = model.get_answer(chat)
    return jsonify({'response': response})
  
  
    return jsonify({'error': 'File upload failed'}), 500


@app.route('/')
def serve_index():
    return send_from_directory('static', 'index.html')

if __name__ == '__main__':
    app.run(debug=True)
```

### 3. Пользовательский Интерфейс

Для удобного взаимодействия с ботом был создан веб-интерфейс. Интерфейс размещен на сервере и доступен через браузер.



![background](https://github.com/medwejonok/RAG_LLM_ChatBot/assets/112614413/b747fad5-0c79-4469-b8fb-d2bdbb06f25c)
![chat](https://github.com/medwejonok/RAG_LLM_ChatBot/assets/112614413/a9fac69b-dee9-4943-8ba8-c3fc2fad181a)

## Тестирование бота
![1](https://github.com/medwejonok/RAG_LLM_ChatBot/assets/112614413/552a3fdb-a340-4121-8fcb-00f53075ac66)
![2](https://github.com/medwejonok/RAG_LLM_ChatBot/assets/112614413/3bc222ed-ef20-4136-ac77-f00f8b07f90f)
![3](https://github.com/medwejonok/RAG_LLM_ChatBot/assets/112614413/0f24f07c-5edd-46f5-afd0-e75f052c7979)
![4](https://github.com/medwejonok/RAG_LLM_ChatBot/assets/112614413/da2e8d34-b078-43dc-a66e-9d63d705b3de)



## Заключение

В рамках выполненного проекта был разработан и внедрен бот, способный квалифицировать входящих лидов, предоставлять первичную консультацию, собирать контактные данные и передавать их менеджеру. Проект включает в себя реализацию логики бота на основе моделей `LangChain` и `DeepInfra`, а также создание веб-интерфейса с использованием Flask.

Дальнейшие шаги могут включать тестирование бота с реальными пользователями, улучшение базы данных с информацией о продукте и оптимизацию модели для повышения точности ответов.
